---
title: "Linear model"
author: "Jose Portilla"
date: "May 13, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Get your data
We will use the Student Performance Data Set from [UC Irvine's Machine Learning Repository!](https://archive.ics.uci.edu/ml/datasets/Student+Performance) Download this data our just use the supplied csv files in the notebook repository. We'll specifically look at the math class (student-mat.csv). Make sure to take note that the delimiter is a semi-colon.

```{r}
# Read data
df <- read.csv('data/student-mat.csv', sep = ';')
head(df)
summary(df)
```

## Attribute Information
Here is the attribute information for our data set: Attribute Information:
Attributes for both student-mat.csv (Math course) and student-por.csv (Portuguese language course) datasets:

1.  school - student's school (binary: 'GP' - Gabriel Pereira or 'MS' - Mousinho da Silveira) 
2.  sex - student's sex (binary: 'F' - female or 'M' - male) 
3.  age - student's age (numeric: from 15 to 22) 
4.  address - student's home address type (binary: 'U' - urban or 'R' - rural) 
5.  famsize - family size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3) 
6.  Pstatus - parent's cohabitation status (binary: 'T' - living together or 'A' - apart) 
7.  Medu - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education or 4 - higher education) 
8.  Fedu - father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education or 4 - higher education) 
9. Mjob - mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other') 
10. Fjob - father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other') 
11. reason - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other') 
12. guardian - student's guardian (nominal: 'mother', 'father' or 'other') 
13. traveltime - home to school travel time (numeric: 1 - less than 15 min, 2 - 15 to 30 min., 3 - 30 min to 1 ho.ur, or 4 - more than 1 hour) 
14. studytime - weekly study time (numeric: 1 - less than 2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - more th.an 10 hours) 
15. failures - number of past class failures (numeric: n if between 1 and 3 , else 4) 
16. schoolsup - extra educational support (binary: yes or no) 
17. famsup - family educational support (binary: yes or no) 
18. paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no) 
19. activities - extra-curricular activities (binary: yes or no) 
20. nursery - attended nursery school (binary: yes or no) 
21. higher - wants to take higher education (binary: yes or no) 
22. internet - Internet access at home (binary: yes or no) 
23. romantic - with a romantic relationship (binary: yes or no) 
24. famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent) 
25. freetime - free time after school (numeric: from 1 - very low to 5 - very high) 
26. goout - going out with friends (numeric: from 1 - very low to 5 - very high) 
27. Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high) 
28. Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high) 
29. health - current health status (numeric: from 1 - very bad to 5 - very good) 
30. absences - number of school absences (numeric: from 0 to 93) 
th.ese grades are related with the course subject, Math or Portuguese:
31. G1 - first period grade (numeric: from 0 to 20) 
31. G2 - second period grade (numeric: from 0 to 20) 
32. G3 - final grade (numeric: from 0 to 20, output target).

## Clean the Data
Next we have to clean this data. This data is actually already cleaned for you, But here are some things you may want to consider doing for other data sets:
Check for NA values
```{r}
# Let's see if we have any NA values:
any(is.na(df))
```

Great! Most real data sets will probably have NA or Null values, so its always good to check! Its up to you how to deal with them, either dropping them if they aren't too many, or imputing other values, like the mean value.

## Categorical Features
Moving on, let's make sure that categorical variables have a factor set to them. For example, the MJob column refers to categories of Job Types, not some numeric value from 1 to 5. R is actually really good at detecting these sort of values and will take of this work for you a lot of the time, but always keep in mind the use of factor() as a possible. Luckily this is basically already, we can check this using the str() function:
```{r}
str(df)
```

## Exploratory Data Analysis
Let's use ggplot2 to explore the data a bit. Feel free to expand on this section:

```{r}
library(ggplot2)
library(ggthemes)
library(dplyr)

```
## Correlation and CorrPlots
From Wikipedia, correlation is defined as:
In statistics, dependence or association is any statistical relationship, whether causal or not, between two random variables or two sets of data. Correlation is any of a broad class of statistical relationships involving dependence, though in common usage it most often refers to the extent to which two variables have a linear relationship with each other. Familiar examples of dependent phenomena include the correlation between the physical statures of parents and their offspring, and the correlation between the demand for a product and its price.
Correlation plots are a great way of exploring data and seeing if there are any interaction terms. Let's start off by just grabbing the numeric data (we can't see correlation for categorical data):
```{r}
# Grab only numeric columns
num.cols <- sapply(df, is.numeric)

# Filter to numeric columns for correlation
cor.data <- cor(df[,num.cols])
cor.data
```


While this is fantastic information, it's hard to take it all in. Let's visualize all this data. There are lots of amazing 3rd party packages to do this, let's use and install the 'corrgram' package and the corrplot package. This will also install a bunch of dependencies for the package.
```{r}
#install.packages('corrgram',repos = 'http://cran.us.r-project.org')
#install.packages('corrplot',repos = 'http://cran.us.r-project.org')
library(corrplot)
library(corrgram)
```

Let's start by using corrplot, the most common one. [Here's a really nice documentation page](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html) on the package. I encourage you to play around with it.

```{r}
corrplot(cor.data, method = "color")
```

Cleary we have very high correlation between G1, G2, and G3 which makes sense since those are grades:

* 31 G1 - first period grade (numeric: from 0 to 20)
* 31 G2 - second period grade (numeric: from 0 to 20)
* 32 G3 - final grade (numeric: from 0 to 20, output target)
Meaning good students do well each period, and poor students do poorly each period, etc. Also a high G1,G2, or G3 value has a negative correlation with failure (number of past class failures).
Also Mother and Father education levels are correlated, which also makes sense.
We can also use the corrgram which allows to just automatically do these type of figures by just passing in the dataframe directly. There's a lot going on here, so reference the [documentation](https://cran.r-project.org/web/packages/corrgram/index.html) of corrgram for more info.
```{r}
corrgram(cor.data, lower.panel = panel.shade, upper.panel = panel.pie, text.panel = panel.txt )
```

Since we're going to eventually try to predict the G3 score let's see a histogram of these scores:
```{r}
ggplot(df, mapping = aes(x = G3)) +
  geom_histogram(bins = 20, alpha = 0.5, fill = "blue") +
  theme_minimal()
```

## Train and Test DataÂ¶
We'll need to split our data into a training set and a testing set in order to test our accuracy. We can do this easily using the caTools library:
```{r}
# Import Library
library(caTools)
# Set a random see so your "random" results are the same as this notebook
set.seed(101) 

# Split up the sample, basically randomly assigns a booleans to a new column "sample"
sample <- sample.split(df$age, SplitRatio = 0.70) # SplitRatio = percent of sample==TRUE

# Training Data
train = subset(df, sample == TRUE)

# Testing Data
test = subset(df, sample == FALSE)
```

## Training our Model
Let's train out model on our training data, then ask for a summary of that model:
```{r}
model <- lm(G3 ~ ., train)
summary(model)
```

Looks like Absences, G1, and G2 scores are good predictors. With age and activities also possibly contributing to a good model.

## Visualize our Model
We can visualize our linear regression model by plotting out the residuals, the residuals are basically a measure of how off we are for each point in the plot versus our model (the error).
```{r}
# Grab residuals
res <- residuals(model)

# Convert to dataframe for ggplot
res <- as.data.frame(res)
head(res)
```

### Using ggplot
```{r}
# Histogram of the residuals
ggplot(res, aes(x = res)) + geom_histogram(fill = "blue", alpha = 0.5)
```

Looks like there are some suspicious residual values that have a value less than -5. We can further explore this by just calling plot on our model. What these plots represent is outside the course of this lecture, but it's covered in ISLR, as well as the Wikipedia page on [Regression Validation](https://en.wikipedia.org/wiki/Regression_validation).

```{r}
plot(model)
```

## Predictions
Let's test our model by predicting on our testing set:
```{r}
G3.predictions <- predict(model, test)
```

Now we can get the root mean squared error, a standardized measure of how off we were with our predicted values:
```{r}
results <- cbind(G3.predictions, test$G3)
colnames(results) <- c("pred", "real")
results <- as.data.frame(results)

```

Now let's take care of negative predictions! Lot's of ways to this, here's a more complicated way, but its a good example of creating a custom function for a custom problem:
```{r}

to_zero <- function(x){
    if  (x < 0){
        return(0)
    }else{
        return(x)
    }
}
results$pred <- sapply(results$pred, to_zero)
```

There's lots of ways to evaluate the prediction values, for example the MSE (mean squared error):
```{r}
mse <- mean((results$real - results$pred)^2)
print(mse)
```

Or the root mean squared error:
```{r}
mse^0.5
```

Or just the R-Squared Value for our model (just for the predictions)
```{r}
SSE = sum((results$pred - results$real)^2)
SST = sum( (mean(df$G3) - results$real)^2)

R2 = 1 - SSE/SST
R2

```

## Conclusion
You should now feel comfortable with the R syntax for a Linear Regression. If some of the plots or math did not make sense to you, make sure to review ISLR and the relevant Wikipedia pages. There is no real substitute for taking the time to read about this material